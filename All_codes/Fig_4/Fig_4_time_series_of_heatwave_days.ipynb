{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "Lo16f_5bcUh-"
   },
   "source": [
    "# Imports and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install netCDF4 h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.feature as cf\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "\n",
    "from cartopy.util import add_cyclic_point\n",
    "from matplotlib import animation\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,timedelta\n",
    "import warnings\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "Tuj-avjYYLM5"
   },
   "source": [
    "# Daily maximums caclulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "lMUveQw7f0mI"
   },
   "source": [
    "Here we are using masked data for defining HW days over Bangladesh. This masked data was created in\n",
    "- `EPT & t2m _masking_for_BD.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the masked data for defining HW over BD only\n",
    "ds_ept=xr.open_dataset('/content/drive/MyDrive/AP_HW/Scripts-ll/HW_dates/data/ept_bdt_masked.nc')\n",
    "ds_t2m=xr.open_dataset('/content/drive/MyDrive/AP_HW/Scripts-ll/HW_dates/data/t2m_bdt_masked.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_ept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ept_daily_max=ds_ept.ept.resample(time='D').max()\n",
    "ds_t2m_daily_max=ds_t2m.t2m.resample(time='D').max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_ept_daily_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_t2m_daily_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "QB_7sxbYtFv3"
   },
   "source": [
    "# Rolling Threshold calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "VtTxoXHbge5p"
   },
   "source": [
    "- To calculate the 95th percentile threshold for each calendar day, we applied a 15-day rolling window centered on that day for the period 1981-2010.\n",
    "- This procedure was repeated for every calendar day and at each grid point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leap day (Feb 29)\n",
    "max_t2m = ds_t2m_daily_max.sel(time=~((ds_t2m_daily_max.time.dt.month == 2) & (ds_t2m_daily_max.time.dt.day == 29)))\n",
    "# max_t2m\n",
    "max_ept = ds_ept_daily_max.sel(time=~((ds_ept_daily_max.time.dt.month == 2) & (ds_ept_daily_max.time.dt.day == 29)))\n",
    "# max_ept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "lZilwq3ihwO9"
   },
   "source": [
    "- Organizing data based on day of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select T2M data for years 1981–2010\n",
    "max_t2m_copy = max_t2m.sel(time=max_t2m.time.dt.year.isin(range(1981,2011)))\n",
    "# Add day-of-year (DOY) coordinate\n",
    "max_t2m_copy['doy'] = max_t2m_copy['time'].dt.dayofyear\n",
    "max_t2m_copy\n",
    "\n",
    "# Select T2M data for years 1981–2010\n",
    "max_ept_copy = max_ept.sel(time=max_ept.time.dt.year.isin(range(1981,2011)))\n",
    "# Add day-of-year (DOY) coordinate\n",
    "max_ept_copy['doy'] = max_ept_copy['time'].dt.dayofyear\n",
    "max_ept_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "ZPOSrwaHh4Fj"
   },
   "source": [
    "- Applying the 15 day rolling window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through 351 days (sliding 15-day windows)\n",
    "\n",
    "t2m_day_group=max_t2m_copy.groupby('doy')\n",
    "t2m_percentile_list=[]\n",
    "for i in range(351):\n",
    "  # Combine data from a 15-day window\n",
    "  combined = xr.concat([t2m_day_group[d] for d in range(i+1,i+16)], dim=\"time\")\n",
    "  # Calculate 95th percentile for each grid cell\n",
    "  percentile_95 = combined.quantile(0.95, dim=\"time\")\n",
    "  # Assign corresponding DOY (centered at i+8)\n",
    "  percentile_95 = percentile_95.assign_coords(doy=8+i)\n",
    "  # Convert to dataset format\n",
    "  percentile_95 = percentile_95.to_dataset()\n",
    "  # Append to list\n",
    "  t2m_percentile_list.append(percentile_95)\n",
    "\n",
    "# Concatenate results into a single dataset along DOY dimension\n",
    "t2m_percentile_doy = xr.concat(t2m_percentile_list, dim='doy')\n",
    "# t2m_percentile_doy\n",
    "\n",
    "\n",
    "# Group EPT by DOY\n",
    "ept_day_group = max_ept_copy.groupby('doy')\n",
    "ept_percentile_list = []\n",
    "\n",
    "# Loop for sliding 15-day window\n",
    "for i in range(351):\n",
    "  # Combine 15 days\n",
    "  combined = xr.concat([ept_day_group[d] for d in range(i+1,i+16)], dim=\"time\")\n",
    "  # 95th percentile\n",
    "  percentile_95 = combined.quantile(0.95, dim=\"time\")\n",
    "  # Assign DOY\n",
    "  percentile_95 = percentile_95.assign_coords(doy=8+i)\n",
    "  # To dataset\n",
    "  percentile_95 = percentile_95.to_dataset()\n",
    "  # Append\n",
    "  ept_percentile_list.append(percentile_95)\n",
    "\n",
    "# Merge EPT percentiles\n",
    "ept_percentile_doy = xr.concat(ept_percentile_list, dim='doy')\n",
    "# ept_percentile_doy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full DOY range 1–365\n",
    "all_doys = np.arange(1, 366)\n",
    "\n",
    "# Reindex the doy dimension to include all days (filling new ones with NaN)\n",
    "t2m_expanded_ds = t2m_percentile_doy.reindex(doy=all_doys)\n",
    "t2m_expanded_ds.to_netcdf('/content/drive/MyDrive/AP_HW/Scripts-ll/HW_dates/data/t2m_percentile_doy.nc')\n",
    "\n",
    "ept_expanded_ds = ept_percentile_doy.reindex(doy=all_doys)\n",
    "ept_expanded_ds.to_netcdf('/content/drive/MyDrive/AP_HW/Scripts-ll/HW_dates/data/ept_percentile_doy.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ept_expanded_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "f-GETg3utFv4"
   },
   "source": [
    "# Data with threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the threshold to each variable dataset to compare.\n",
    "\n",
    "t2m_year_group=max_t2m.groupby('time.year')\n",
    "t2m_yearly_list=[]\n",
    "for year in range(1971,2025):\n",
    "  percentile_array = xr.DataArray(\n",
    "      data=t2m_expanded_ds.t2m.values,  # Ensure shape matches\n",
    "      coords=t2m_year_group[year].coords,\n",
    "      dims=t2m_year_group[year].dims\n",
    "  )\n",
    "  # Add new variable\n",
    "  year_group_ds = t2m_year_group[year].to_dataset()  # Convert to Dataset\n",
    "  year_group_ds[\"95th_t2m\"] = percentile_array  # Assign the new array\n",
    "  t2m_yearly_list.append(year_group_ds)\n",
    "t2m_max_data=xr.concat(t2m_yearly_list,dim='time')\n",
    "# t2m_max_data\n",
    "\n",
    "ept_year_group=max_ept.groupby('time.year')\n",
    "ept_yearly_list=[]\n",
    "for year in range(1971,2025):\n",
    "  percentile_array = xr.DataArray(\n",
    "      data=ept_expanded_ds.ept.values,  # Ensure shape matches\n",
    "      coords=ept_year_group[year].coords,\n",
    "      dims=ept_year_group[year].dims\n",
    "  )\n",
    "  # Add new variable\n",
    "  year_group_ds = ept_year_group[year].to_dataset()  # Convert to Dataset\n",
    "  year_group_ds[\"95th_ept\"] = percentile_array  # Assign the new array\n",
    "  ept_yearly_list.append(year_group_ds)\n",
    "ept_max_data=xr.concat(ept_yearly_list,dim='time')\n",
    "# ept_max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ept_expanded_ds.ept.shape,ept_year_group[1971].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "CLO2wQ3KtFwB"
   },
   "source": [
    "# Warm days (days surpassing the thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m_march_september=t2m_max_data.sel(time=t2m_max_data.time.dt.month.isin([3,4,5,6,7,8,9]))\n",
    "ept_march_september=ept_max_data.sel(time=ept_max_data.time.dt.month.isin([3,4,5,6,7,8,9]))\n",
    "\n",
    "# # t2m_march_september\n",
    "# # ept_march_september"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t2m=t2m_march_september.to_dataframe().reset_index()\n",
    "df_ept=ept_march_september.to_dataframe().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_t2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_t2m,df_ept)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['warm_days']=0\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    " # checking the threshold conditions\n",
    "\n",
    "hw_condition= (df['ept']>df['95th_ept'])|(df['t2m']>df['95th_t2m'])\n",
    "df.loc[hw_condition,'warm_days']=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['warm_days']==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['warm_days'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "nTC8rypitFwC"
   },
   "source": [
    "## Heatwave days List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['heatwave']=df['warm_days']\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['heatwave'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time']=pd.to_datetime(df['time'])\n",
    "dates=df.time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Heatwave Definition: minimum 3 consecutive days and 75 grids spread over BD #####\n",
    "\n",
    "iteration = 0\n",
    "starting_date = []\n",
    "duration = []\n",
    "ending_date = []\n",
    "temporary_list = []\n",
    "\n",
    "# Loop through all dates\n",
    "for i in tqdm(range(len(dates)-1), desc='Dates', leave=True):\n",
    "    # Count grids marked as warm days\n",
    "    grid_count = (df[df['time'] == dates[i]]['warm_days'].values == 1).sum()\n",
    "    # Check if current day is consecutive\n",
    "    consecutive_day = (dates[i] - dates[i-1]).days == 1\n",
    "\n",
    "    # If condition met: at least 75 grids and consecutive day\n",
    "    if grid_count >= 75 and consecutive_day:\n",
    "        iteration += 1\n",
    "        temporary_list.append(dates[i])\n",
    "    else:\n",
    "        # If <3 days, reset heatwave flag\n",
    "        if iteration <= 2:\n",
    "            condition_1 = (df['time'] >= dates[i-iteration]) & (df['time'] <= dates[i])\n",
    "            df.loc[condition_1, 'heatwave'] = 0\n",
    "        # If ≥3 days, record as heatwave\n",
    "        if iteration >= 3:\n",
    "            starting_date.append(temporary_list[0])\n",
    "            ending_date.append(temporary_list[-1])\n",
    "            duration.append(iteration)\n",
    "            condition_3 = (df['time'] == dates[i])\n",
    "            df.loc[condition_3, 'heatwave'] = 0\n",
    "        # Reset counters\n",
    "        iteration = 0\n",
    "        temporary_list = []\n",
    "\n",
    "# Print counts of detected events\n",
    "print(len(starting_date), len(ending_date), len(duration))\n",
    "\n",
    "##### end #####\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Starting_Date': starting_date, 'Ending_Date': ending_date,'Duration':duration})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the dataframe for heatwave dates.\n",
    "df = df.to_csv('/content/drive/MyDrive/AP_HW/Scripts-ll/HW_dates/data/HW_list_rolling_def.csv',index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "oEFCPeYIVAdm"
   },
   "source": [
    "## Import the file to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/content/drive/MyDrive/AP_HW/Scripts-ll/HW_dates/data/HW_list_rolling_def.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Starting_Date'] = pd.to_datetime(df[\"Starting_Date\"])\n",
    "df['Ending_Date'] = pd.to_datetime(df[\"Ending_Date\"])\n",
    "df[df['Starting_Date'].dt.year == 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time difference and store in a new column as the difference in days\n",
    "# we are gonna merge any hw events separated by 2 days\n",
    "df['Next_Row_Difference'] = (df['Starting_Date'].shift(-1) - df['Ending_Date']).dt.days\n",
    "df=df.fillna(0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to group rows where the difference is <= 2 or having 1 day gap\n",
    "df['Flag'] = (~(df['Next_Row_Difference'] <=2))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the number counter\n",
    "current_number = 1\n",
    "\n",
    "# Iterate through the 'Flag' column\n",
    "for i, value in enumerate(df['Flag']):\n",
    "    if value:  # For True values\n",
    "        df.at[i, 'Group'] = current_number\n",
    "        current_number += 1  # Increment the number for each True\n",
    "    else:  # For False values\n",
    "        df.at[i, 'Group'] = current_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[(df['Starting_Date'].dt.year == 2023) | (df['Starting_Date'].dt.year == 2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge rows in each group\n",
    "df_merged = df.groupby('Group', as_index=False).agg({\n",
    "    'Starting_Date': 'first',  # Take the first starting date in the group\n",
    "    'Ending_Date': 'last',  # Take the last ending date in the group\n",
    "})\n",
    "\n",
    "# View the resulting merged DataFrame\n",
    "df_merged.drop('Group',axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Duration']=(df_merged['Ending_Date'] - df_merged['Starting_Date']).dt.days +1\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged['Starting_Date'].dt.year == 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "fCB6t7rNUEX9"
   },
   "source": [
    "# Calculation by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_merged\n",
    "df['Starting_Date']=pd.to_datetime(df[\"Starting_Date\"])\n",
    "df['Year']=df['Starting_Date'].dt.year\n",
    "yearly_sum=df.groupby('Year')['Duration'].sum().reset_index()\n",
    "\n",
    "grouped = df.groupby('Year')\n",
    "yearly_event_count=list(grouped.size())\n",
    "len(yearly_event_count)\n",
    "\n",
    "\n",
    "yearly_sum['total_events'] = yearly_event_count\n",
    "\n",
    "pre_monsoon=df[(df['Starting_Date'].dt.month >= 3) & (df['Starting_Date'].dt.month <= 6)]\n",
    "# pre_monsoon\n",
    "\n",
    "yearly_pre_monsoon=pre_monsoon.groupby('Year')['Duration'].sum().reset_index()\n",
    "\n",
    "all_years=range(1971,2024)\n",
    "missing_years=set(all_years)-set(yearly_pre_monsoon['Year'])\n",
    "missing_years_df=pd.DataFrame({'Year':list(missing_years),'Duration':0,'total_events':0})\n",
    "yearly_pre_monsoon=pd.concat([yearly_pre_monsoon,missing_years_df],ignore_index=True)\n",
    "yearly_pre_monsoon=yearly_pre_monsoon.sort_values(by='Year').reset_index(drop=True)\n",
    "\n",
    "#Monsoon\n",
    "moonson=df[(df['Starting_Date'].dt.month>=7)&(df['Starting_Date'].dt.month<=9)]\n",
    "yearly_moonson=moonson.groupby('Year')['Duration'].sum().reset_index()\n",
    "all_years=range(1971,2024)\n",
    "missing_years=set(all_years)-set(yearly_moonson['Year'])\n",
    "missing_years_df=pd.DataFrame({'Year':list(missing_years),\n",
    "                              'Duration':0,'total_events':0})\n",
    "yearly_moonson=pd.concat([yearly_moonson,missing_years_df],ignore_index=True)\n",
    "yearly_moonson=yearly_moonson.sort_values(by=\"Year\").reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal.windows import gaussian\n",
    "from scipy.ndimage import convolve1d\n",
    "# Create a Gaussian kernel\n",
    "window_size = 20  # Length of the kernel\n",
    "sigma = 3.5\n",
    "gaussian_kernel = gaussian(window_size, std=sigma)\n",
    "gaussian_kernel /= gaussian_kernel.sum()  # Normalize kernel\n",
    "gaussian_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "NRADFS18ccCo"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal.windows import gaussian\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "# Specify the size of the figure (width, height) in inches\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# Assuming yearly data is available\n",
    "tickname = list(yearly_moonson['Year'])\n",
    "\n",
    "x = np.arange(len(tickname))  # Create indices for the x-axis\n",
    "y = np.arange(60)\n",
    "\n",
    "y1 = yearly_pre_monsoon['Duration']\n",
    "y2 = yearly_moonson['Duration']\n",
    "\n",
    "#  Gaussian kernel\n",
    "window_size = 20  # Length of the kernel\n",
    "sigma = 3.5\n",
    "gaussian_kernel = gaussian(window_size, std=sigma)\n",
    "gaussian_kernel /= gaussian_kernel.sum()  # Normalize kernel\n",
    "\n",
    "# Gaussian smoothing\n",
    "smooth_y1 = convolve1d(y1, gaussian_kernel, mode='reflect')\n",
    "smooth_y2 = convolve1d(y2, gaussian_kernel, mode='reflect')\n",
    "\n",
    "# Plot the original data\n",
    "ax.plot(tickname, y1, alpha=0.3, color='red')\n",
    "ax.plot(tickname, y2, alpha=0.3, color='purple')\n",
    "\n",
    "# Plot the smoothed data\n",
    "ax.plot(tickname, smooth_y1, color='red', linewidth=2, label='Pre-monsoon')\n",
    "ax.plot(tickname, smooth_y2, color='purple', linewidth=2, label='Monsoon')\n",
    "\n",
    "# Adding a vertical line at the year 1980 and 2023\n",
    "baseline=2000\n",
    "endline=2024\n",
    "ax.axvline(x=baseline, color='gray', linestyle='--', linewidth=1.5)\n",
    "ax.axvline(x=endline, color='gray', linestyle='--', linewidth=1.5)\n",
    "\n",
    "# Adding intersection points\n",
    "ax.scatter(baseline, smooth_y1[baseline-1971], color='red', s=40)\n",
    "ax.scatter(endline, smooth_y1[endline-1971], color='red', marker='*', s=40)\n",
    "ax.scatter(baseline, smooth_y2[baseline-1971], color='purple', s=40)\n",
    "ax.scatter(endline, smooth_y2[endline-1971], color='purple', marker='*', s=40)\n",
    "\n",
    "# Calculating the difference between 2023 and 2000\n",
    "pre_monsoon_rise = smooth_y1[endline-1971] - smooth_y1[baseline-1971]\n",
    "monsoon_rise = smooth_y2[endline-1971] - smooth_y2[baseline-1971]\n",
    "\n",
    "# text to highlight trends\n",
    "ax.text(2028, 32, f'+{pre_monsoon_rise} days', fontsize=12, color='red', ha='center', fontweight='bold')\n",
    "ax.text(2028, 42, f'+{monsoon_rise} days', fontsize=12, color='purple', ha='center', fontweight='bold')\n",
    "ax.text(2029.2, 52, 'Mean change', fontsize=12, color='black', ha='center', fontweight='bold')\n",
    "ax.text(2029, 47, f'{endline} vs. {baseline}', fontsize=11, color='black', ha='center')\n",
    "\n",
    "# Removing top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# xticks and labels with rotation\n",
    "ax.set_xticks(tickname[::2])\n",
    "ax.set_xticklabels(tickname[::2], rotation=45)\n",
    "# xticks and labels with rotation\n",
    "yticks=np.arange(0,56)\n",
    "ax.set_yticks(yticks[::5])\n",
    "ax.set_yticklabels(yticks[::5])\n",
    "\n",
    "# Display the legend (only for smoothed lines)\n",
    "ax.legend(loc='center',bbox_to_anchor=(0.5, 1.0), fontsize=10,ncol=2)\n",
    "ax.grid(axis='y', linestyle='--')\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Heatwave Days per year')\n",
    "plt.title('Time Series and Trends of Heatwave Days by Season',y=1.05,fontsize=11)\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "fig.savefig(\"/content/drive/MyDrive/AP_HW/Scripts-ll/Manuscript codes/All_Figures/Fig_4_time_series_of_heatwave_days.jpg\",\n",
    "            dpi=300, format=\"jpg\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
